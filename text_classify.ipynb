{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2098c5b4",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "This dataset originates on Reddit but I got it as one of the Kaggle NLP data sets.\n",
    "It is reddit posts that have been labeled as either related to depression or not.\n",
    "\n",
    "Given that September is Suicide Awareness month this seemed like a good data set  \n",
    "to start my NLP journey.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a4c2117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 16:07:45.653909: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-28 16:07:45.793309: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-28 16:07:46.280150: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
      "2022-09-28 16:07:46.280206: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
      "2022-09-28 16:07:46.280211: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef73a01",
   "metadata": {},
   "source": [
    "## Data\n",
    "The dataset is actually a CSV file with one column for the text and another for the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b41fbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7731, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>is_depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we understand that most people who reply immed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>welcome to r depression s check in post a plac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anyone else instead of sleeping more when depr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  is_depression\n",
       "0  we understand that most people who reply immed...              1\n",
       "1  welcome to r depression s check in post a plac...              1\n",
       "2  anyone else instead of sleeping more when depr...              1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\"depression_dataset_reddit_cleaned.csv\")\n",
    "print(data.shape)\n",
    "display(data.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c7568",
   "metadata": {},
   "source": [
    "## Cleaning the text.\n",
    "One of the Kaggle code examples had the code below for \"cleaning\" the text.  \n",
    "I actually tried using it but it seemed to make the text less legible.  \n",
    "\n",
    "Perhaps given that the label for the text is \"clean_text\"  \n",
    "it might be that such cleaning was needed originally   \n",
    "but then someone posted a \"cleaned\" version of the text.\n",
    "\n",
    "I don't know, but I did not use the clean function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceef0816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/john/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# I copied this from one of the Kaggle submissions\n",
    "nltk.download(\"stopwords\")\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "stopword=set(stopwords.words('english'))\n",
    "def clean(text):\n",
    "    assert(False) # do not use this function\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = [word for word in text.split(' ') if word not in stopword]\n",
    "    text=\" \".join(text)\n",
    "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
    "    text=\" \".join(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9b0b4a",
   "metadata": {},
   "source": [
    "## Split into train and val subsets\n",
    "I have used the sklearn function to do this,  \n",
    "but here I just use numpy and pandas to acheieve the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cad5e4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7731  6184  1547\n",
      "(6184, 2) (1547, 2)\n"
     ]
    }
   ],
   "source": [
    "#  First split the indices\n",
    "all_idx = data.index\n",
    "train_size = int(np.floor(0.8*data.shape[0]))\n",
    "train_idx = np.random.choice(data.index, train_size, replace=False)\n",
    "# take the difference of original and train to get val\n",
    "val_idx = list(set(all_idx).difference(set(train_idx)))\n",
    "print(f\"{len(all_idx)}  {len(train_idx)}  {len(val_idx)}\")\n",
    "\n",
    "# and now use the indices to get the data sets\n",
    "train = data.loc [train_idx].copy()\n",
    "val = data.loc[val_idx].copy()\n",
    "print(train.shape, val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6552a344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>is_depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5649</th>\n",
       "      <td>ilovedt that s what i thought bummer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3709</th>\n",
       "      <td>adewunmitemit 9 weirdpeace olumurewa the sound...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6781</th>\n",
       "      <td>isnt very happy with twitter at the moment won...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_text  is_depression\n",
       "5649               ilovedt that s what i thought bummer              0\n",
       "3709  adewunmitemit 9 weirdpeace olumurewa the sound...              1\n",
       "6781  isnt very happy with twitter at the moment won...              0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>is_depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we understand that most people who reply immed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anyone else instead of sleeping more when depr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i ve been struggling with depression for a lon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           clean_text  is_depression\n",
       "0   we understand that most people who reply immed...              1\n",
       "2   anyone else instead of sleeping more when depr...              1\n",
       "10  i ve been struggling with depression for a lon...              1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# have a look at the head of each dataset\n",
    "display(train.head(3))\n",
    "display(val.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d92da",
   "metadata": {},
   "source": [
    "## Parameters for the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71c868ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I did not do tests with changing these\n",
    "# I took them from a coursera course on NLP with tensorflow.\n",
    "\n",
    "# Vocabulary size of the tokenizer\n",
    "vocab_size = 10000\n",
    "\n",
    "# Maximum length of the padded sequences\n",
    "max_length = 32\n",
    "\n",
    "# Output dimensions of the Embedding layer\n",
    "embedding_dim = 16\n",
    "\n",
    "\n",
    "# Parameters for padding and OOV tokens\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ccc2b3",
   "metadata": {},
   "source": [
    "## Final setup\n",
    "Run the tokenizer to get the sequences for train and val\n",
    "as well as the labels for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf0bf239",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the Tokenizer class\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "\n",
    "# Generate the word index dictionary for the training sentences\n",
    "tokenizer.fit_on_texts(train[\"clean_text\"])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Generate and pad the training sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train[\"clean_text\"])\n",
    "train_padded = pad_sequences(train_sequences,maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "# Generate and pad the test sequences\n",
    "val_sequences = tokenizer.texts_to_sequences(val[\"clean_text\"])\n",
    "val_padded = pad_sequences(val_sequences,maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "# Convert the labels lists into numpy arrays\n",
    "train_labels = np.array(train[\"is_depression\"])\n",
    "val_labels = np.array(val[\"is_depression\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d7b68",
   "metadata": {},
   "source": [
    "## The model\n",
    "The model is fairly simple.\n",
    "* a single embedding layer  \n",
    "* a flattening layer  \n",
    "* Relu\n",
    "* and a sigmoid for the binary prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d5f7c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 16:07:47.756723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-28 16:07:47.781255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-28 16:07:47.781399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-28 16:07:47.781889: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-28 16:07:47.782605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-28 16:07:47.782734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-28 16:07:47.782840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-28 16:07:48.196970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-28 16:07:48.197137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-28 16:07:48.197250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-28 16:07:48.197346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4179 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Setup the training parameters\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=.0015),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c3a988e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 32, 16)            160000    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 3078      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 163,085\n",
      "Trainable params: 163,085\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0015>\n"
     ]
    }
   ],
   "source": [
    "# Print the model summary\n",
    "model.summary()\n",
    "print(model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4a25cd",
   "metadata": {},
   "source": [
    "## run the model\n",
    "I tried a few variaions on epochs.  \n",
    "since every run tends to be different,  \n",
    "but I found that I got pretty good results  \n",
    "with the number of epochs between 6 and 12.\n",
    "\n",
    "That is, with the validation accuracy.  \n",
    "The train accuracy seemed pretty good with all the runs.\n",
    "\n",
    "I also tried a few different learning rates.  \n",
    "The default for Adam is .001, and I tried .01  \n",
    "and .0015.   \n",
    "With this small data set and the small number of epochs  \n",
    "I did not see a huge difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0ee9bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "194/194 [==============================] - 1s 2ms/step - loss: 0.3946 - accuracy: 0.8554 - val_loss: 0.2536 - val_accuracy: 0.9024\n",
      "Epoch 2/10\n",
      "194/194 [==============================] - 0s 2ms/step - loss: 0.1930 - accuracy: 0.9203 - val_loss: 0.1400 - val_accuracy: 0.9580\n",
      "Epoch 3/10\n",
      "194/194 [==============================] - 0s 2ms/step - loss: 0.0781 - accuracy: 0.9762 - val_loss: 0.0967 - val_accuracy: 0.9670\n",
      "Epoch 4/10\n",
      "194/194 [==============================] - 0s 2ms/step - loss: 0.0289 - accuracy: 0.9916 - val_loss: 0.0928 - val_accuracy: 0.9638\n",
      "Epoch 5/10\n",
      "194/194 [==============================] - 0s 2ms/step - loss: 0.0152 - accuracy: 0.9958 - val_loss: 0.0938 - val_accuracy: 0.9683\n",
      "Epoch 6/10\n",
      "194/194 [==============================] - 0s 2ms/step - loss: 0.0099 - accuracy: 0.9969 - val_loss: 0.1026 - val_accuracy: 0.9664\n",
      "Epoch 7/10\n",
      "194/194 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 0.9989 - val_loss: 0.1100 - val_accuracy: 0.9664\n",
      "Epoch 8/10\n",
      "194/194 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9994 - val_loss: 0.1204 - val_accuracy: 0.9670\n",
      "Epoch 9/10\n",
      "194/194 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 0.1195 - val_accuracy: 0.9651\n",
      "Epoch 10/10\n",
      "194/194 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.1267 - val_accuracy: 0.9638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f208013dd80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_padded, train_labels, epochs=num_epochs, validation_data=(val_padded, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f824139d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "With this small number of epochs it was not clear if I was overfitting.  \n",
    "On some it seemed so, but on others not.  \n",
    "\n",
    "Maybe in my next project I will use a larger dataset and also \n",
    "try using some hyper parameter tuning tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "145faa05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-28 16:07:52.847285\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tftext",
   "language": "python",
   "name": "tftext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
