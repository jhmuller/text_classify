{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2098c5b4",
   "metadata": {},
   "source": [
    "# What\n",
    "Classify text.  In this case, binary classification.\n",
    "\n",
    "\n",
    "# Why\n",
    "There are lots of applications for text classification, \n",
    "e.g. is the text offensive, is it potentially a scam, et cetera.\n",
    "In this case the text is Reddit posts and the question is whether\n",
    "it involves depression.  Again I can easily imagine uses for this\n",
    "for text and email as early warning signs, although there are \n",
    "privacy challenges there.\n",
    "\n",
    "\n",
    "# Background\n",
    "\n",
    "I have been getting more comfortable with text applications so \n",
    "I wanted something to show that.\n",
    "This dataset originates on Reddit but I got it as one of the Kaggle NLP data sets.\n",
    "It is reddit posts that have been labeled as either related to depression or not.\n",
    "\n",
    "Given that September is Suicide Awareness month this seemed like a good data set  \n",
    "to start my NLP journey.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a4c2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef73a01",
   "metadata": {},
   "source": [
    "## Data\n",
    "The dataset is actually a CSV file with one column for the text and another for the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7b41fbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7731, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>is_depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we understand that most people who reply immed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>welcome to r depression s check in post a plac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anyone else instead of sleeping more when depr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  is_depression\n",
       "0  we understand that most people who reply immed...              1\n",
       "1  welcome to r depression s check in post a plac...              1\n",
       "2  anyone else instead of sleeping more when depr...              1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\"depression_dataset_reddit_cleaned.csv\")\n",
    "print(data.shape)\n",
    "display(data.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c7568",
   "metadata": {},
   "source": [
    "## Cleaning the text.\n",
    "One of the Kaggle code examples had the code below for \"cleaning\" the text.  \n",
    "I actually tried using it but it seemed to make the text less legible.  \n",
    "\n",
    "Perhaps given that the label for the text is \"clean_text\"  \n",
    "it might be that such cleaning was needed originally   \n",
    "but then someone posted a \"cleaned\" version of the text.\n",
    "\n",
    "I don't know, but I did not use the clean function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ceef0816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/john/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# I copied this from one of the Kaggle submissions\n",
    "nltk.download(\"stopwords\")\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "stopword=set(stopwords.words('english'))\n",
    "def clean(text):\n",
    "    assert(False) # do not use this function\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = [word for word in text.split(' ') if word not in stopword]\n",
    "    text=\" \".join(text)\n",
    "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
    "    text=\" \".join(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9b0b4a",
   "metadata": {},
   "source": [
    "## Split into train and val subsets\n",
    "I have used the sklearn function to do this, but in this case\n",
    "I prefered using pandas and numpy to keep the data as data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cad5e4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train shape (6184, 2), val shape (773, 2),  test shape (774, 2)\n"
     ]
    }
   ],
   "source": [
    "#  First split the indices\n",
    "all_idx = data.index\n",
    "train_size = int(np.floor(0.8*data.shape[0]))\n",
    "train_idx = np.random.choice(data.index, train_size, replace=False)\n",
    "\n",
    "# take the difference of original and train to get val\n",
    "other_idx = list(set(all_idx).difference(set(train_idx)))\n",
    "\n",
    "val_size = int(np.floor(0.5*len(other_idx)))\n",
    "val_idx = np.random.choice(other_idx, val_size, replace=False)\n",
    "\n",
    "test_idx = list(set(other_idx).difference(set(val_idx)))\n",
    "\n",
    "# and now use the indices to get the data sets\n",
    "train = data.loc [train_idx].copy()\n",
    "val = data.loc[val_idx].copy()\n",
    "test = data.loc[test_idx].copy()\n",
    "print(f\" train shape {train.shape}, val shape {val.shape},  test shape {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6552a344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>is_depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4636</th>\n",
       "      <td>mtsiaklides aw i wish i could i can t really s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3492</th>\n",
       "      <td>heartbreaking to see kid taking their life out...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>maybe i should have been locked away for the r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_text  is_depression\n",
       "4636  mtsiaklides aw i wish i could i can t really s...              0\n",
       "3492  heartbreaking to see kid taking their life out...              1\n",
       "2332  maybe i should have been locked away for the r...              1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>is_depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6594</th>\n",
       "      <td>http twitpic com y z see where we ve been move...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515</th>\n",
       "      <td>for about a week now i ve been experiencing ex...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>someone pls tell me how to get over this i m c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_text  is_depression\n",
       "6594  http twitpic com y z see where we ve been move...              0\n",
       "1515  for about a week now i ve been experiencing ex...              1\n",
       "209   someone pls tell me how to get over this i m c...              1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " % true in train 0.5\n",
      " % true in val 0.48\n"
     ]
    }
   ],
   "source": [
    "# have a look at the head of each dataset\n",
    "display(train.head(3))\n",
    "display(val.head(3))\n",
    "print(f\" % true in train {np.round(train['is_depression'].sum()/train.shape[0], 2)}\")\n",
    "print(f\" % true in val {np.round(val['is_depression'].sum()/val.shape[0], 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d92da",
   "metadata": {},
   "source": [
    "## Parameters for the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "71c868ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary size of the tokenizer\n",
    "vocab_size = 10000\n",
    "\n",
    "# Maximum length of the padded sequences\n",
    "max_length = 100\n",
    "\n",
    "# Output dimensions of the Embedding layer\n",
    "embedding_dim = 10\n",
    "\n",
    "\n",
    "# Parameters for padding and OOV tokens\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ccc2b3",
   "metadata": {},
   "source": [
    "## Final setup\n",
    "Run the tokenizer to get the sequences for train and val\n",
    "as well as the labels for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bf0bf239",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the Tokenizer class\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "\n",
    "# Generate the word index dictionary for the training sentences\n",
    "tokenizer.fit_on_texts(train[\"clean_text\"])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Generate and pad the training sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train[\"clean_text\"])\n",
    "train_padded = pad_sequences(train_sequences,maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "# Generate and pad the val sequences\n",
    "val_sequences = tokenizer.texts_to_sequences(val[\"clean_text\"])\n",
    "val_padded = pad_sequences(val_sequences,maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "# Generate and pad the test sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test[\"clean_text\"])\n",
    "test_padded = pad_sequences(test_sequences,maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "# Convert the labels lists into numpy arrays\n",
    "train_labels = np.array(train[\"is_depression\"])\n",
    "val_labels = np.array(val[\"is_depression\"])\n",
    "test_labels = np.array(test[\"is_depression\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d7b68",
   "metadata": {},
   "source": [
    "## The model\n",
    "The model is fairly simple.\n",
    "* a single embedding layer  \n",
    "* a flattening layer  \n",
    "* A Dense layer with Relu activation\n",
    "* A Dense layer with sigmoid for the binary prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6d5f7c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Setup the training parameters\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=.0015),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4c3a988e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 100, 10)           100000    \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 4)                 4004      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 104,009\n",
      "Trainable params: 104,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0015>\n"
     ]
    }
   ],
   "source": [
    "# Print the model summary\n",
    "model.summary()\n",
    "print(model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4a25cd",
   "metadata": {},
   "source": [
    "## run the model\n",
    "I tried a few variaions on epochs.  \n",
    "since every run tends to be different,  \n",
    "but I found that I got pretty good results  \n",
    "with the number of epochs between 6 and 12.\n",
    "\n",
    "That is, with the validation accuracy.  \n",
    "The train accuracy seemed pretty good with all the runs.\n",
    "\n",
    "I also tried a few different learning rates.  \n",
    "The default for Adam is .001, and I tried .01  \n",
    "and .0015.   \n",
    "With this small data set and the small number of epochs  \n",
    "I did not see a huge difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f0ee9bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "194/194 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.1755 - val_accuracy: 0.9586\n",
      "Epoch 2/10\n",
      "194/194 [==============================] - 0s 2ms/step - loss: 9.0905e-04 - accuracy: 0.9998 - val_loss: 0.1838 - val_accuracy: 0.9586\n",
      "Epoch 3/10\n",
      "194/194 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.1939 - val_accuracy: 0.9521\n",
      "Epoch 4/10\n",
      "194/194 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.1878 - val_accuracy: 0.9521\n",
      "Epoch 5/10\n",
      "194/194 [==============================] - 0s 2ms/step - loss: 8.7833e-04 - accuracy: 0.9998 - val_loss: 0.2051 - val_accuracy: 0.9573\n",
      "Epoch 6/10\n",
      "194/194 [==============================] - 0s 2ms/step - loss: 7.1931e-04 - accuracy: 0.9998 - val_loss: 0.1819 - val_accuracy: 0.9586\n",
      "Epoch 7/10\n",
      "194/194 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.1965 - val_accuracy: 0.9547\n",
      "Epoch 8/10\n",
      "194/194 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.2057 - val_accuracy: 0.9547\n",
      "Epoch 9/10\n",
      "194/194 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.2119 - val_accuracy: 0.9534\n",
      "Epoch 10/10\n",
      "194/194 [==============================] - 0s 2ms/step - loss: 9.5017e-04 - accuracy: 0.9998 - val_loss: 0.2080 - val_accuracy: 0.9534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb8ba26dbd0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_padded, train_labels, epochs=num_epochs, validation_data=(val_padded, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f824139d",
   "metadata": {},
   "source": [
    "## Comments on training\n",
    "\n",
    "The model appears to be overfitting as the val_loss is considerably higher than the train_loss.\n",
    "\n",
    "Still the accuracy of the validation set is 96% which might be good enough for some applications.\n",
    "\n",
    "Let's see how it does on the holdout test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "542ad9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2321 - accuracy: 0.9561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2321440577507019, 0.9560723304748535]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_padded, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59287956",
   "metadata": {},
   "source": [
    "## Test performance\n",
    "Numbers for the test set are in line with the validation set, which is not surprising since we did not actually use the validation set in training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d118134d",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Building a text classifier with Keras is fairly straightforward.\n",
    "I did not experiment too much with the hyperparameters in part because the ones I choose at the start performed fairly well on the training set.\n",
    "\n",
    "For more challenging data sets, the Keras Tuner looks like an easy-to-use option for \n",
    "searching over the parameter space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tftext",
   "language": "python",
   "name": "tftext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
